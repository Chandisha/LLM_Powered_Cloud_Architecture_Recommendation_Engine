{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "import json, getpass, pathlib\n",
        "import pathlib, subprocess, zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "import textwrap, html\n",
        "from IPython.display import display, Markdown\n",
        "from functools import lru_cache\n",
        "import yaml\n",
        "from huggingface_hub import InferenceClient\n",
        "from IPython.display import display, Markdown\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "4Lnz3pHf72WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Hugging Face API Token\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "if not token:\n",
        "    # fallback: prompt user inside notebook\n",
        "    token = input(\"Enter your Hugging Face API token: \").strip()\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
        "\n",
        "print(\"✅ Hugging Face API key loaded\")\n"
      ],
      "metadata": {
        "id": "8t22iYEF_bRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3:  Setup Hugging Face LLM (no fallback)\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Example model: Falcon-7B-Instruct (fast + free-tier friendly)\n",
        "try:\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=os.getenv(\"HF_LLM_MODEL\", \"tiiuae/falcon-7b-instruct\"),\n",
        "        model_kwargs={\"temperature\":0.6, \"max_new_tokens\":512}\n",
        "    )\n",
        "    print(\"✅ Hugging Face LLM loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Failed to load LLM: {e}\")"
      ],
      "metadata": {
        "id": "JLFc5ZSx_dBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: One-time Kaggle API credentials (improved)\n",
        "import json, getpass, pathlib\n",
        "\n",
        "kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "kaggle_json = kaggle_dir / \"kaggle.json\"\n",
        "\n",
        "if not kaggle_json.exists():\n",
        "    username = os.getenv(\"KAGGLE_USERNAME\")\n",
        "    key = os.getenv(\"KAGGLE_KEY\")\n",
        "    if not username or not key:\n",
        "        print(\"Enter Kaggle credentials (leave blank to skip if already configured):\")\n",
        "        username = input(\"Kaggle Username: \").strip()\n",
        "        key = getpass.getpass(\"Kaggle API Key: \").strip()\n",
        "    if username and key:\n",
        "        kaggle_json.write_text(json.dumps({\"username\": username, \"key\": key}))\n",
        "        os.chmod(kaggle_json, 0o600)\n",
        "        print(\"✅ Saved ~/.kaggle/kaggle.json\")\n",
        "    else:\n",
        "        print(\"ℹ️ Skipped writing kaggle.json (assuming already configured).\")\n",
        "else:\n",
        "    print(\"✅ Found existing ~/.kaggle/kaggle.json\")\n"
      ],
      "metadata": {
        "id": "R6ZiSRL8_ix5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Download & unzip Kaggle dataset (Stack Overflow: stacksample)\n",
        "import pathlib, subprocess, zipfile\n",
        "\n",
        "DATA_DIR = pathlib.Path(\"./data\")\n",
        "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "zip_path = DATA_DIR / \"stacksample.zip\"\n",
        "if not zip_path.exists():\n",
        "    subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"stackoverflow/stacksample\", \"-p\", \"./data\", \"-q\"])\n",
        "\n",
        "if zip_path.exists():\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(DATA_DIR)\n",
        "    print(\"✅ Extracted stacksample\")\n",
        "else:\n",
        "    print(\"⚠️ Could not find the downloaded zip. Ensure Kaggle config is correct.\")\n",
        "\n",
        "print(\"Files:\", [p.name for p in DATA_DIR.iterdir()])"
      ],
      "metadata": {
        "id": "K_VkSVk4_kv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load Stack Overflow sample data; prepare a small tag-index for retrieval\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "q_path = Path(\"./data/Questions.csv\")\n",
        "a_path = Path(\"./data/Answers.csv\")\n",
        "t_path = Path(\"./data/Tags.csv\")\n",
        "\n",
        "questions = pd.read_csv(q_path, encoding=\"latin-1\")\n",
        "answers = pd.read_csv(a_path, encoding=\"latin-1\")\n",
        "tags = pd.read_csv(t_path, encoding=\"latin-1\")\n",
        "\n",
        "questions = questions.rename(columns={\"Id\":\"QuestionId\",\"OwnerUserId\":\"UserId\"})\n",
        "answers = answers.rename(columns={\"Id\":\"AnswerId\",\"OwnerUserId\":\"UserId\"})\n",
        "\n",
        "tag_map = tags.groupby(\"Id\")[\"Tag\"].apply(list).rename(\"TagList\")\n",
        "questions = questions.merge(tag_map, left_on=\"QuestionId\", right_index=True, how=\"left\")\n",
        "\n",
        "ARCH_TAGS = {\n",
        "    \"architecture\",\"design\",\"microservices\",\"monolith\",\"rest\",\"graphql\",\"event-driven\",\n",
        "    \"serverless\",\"kubernetes\",\"docker\",\"kafka\",\"rabbitmq\",\"cassandra\",\"redis\",\n",
        "    \"mongodb\",\"postgresql\",\"mysql\",\"elasticsearch\",\"aws\",\"gcp\",\"azure\",\"scalability\",\n",
        "    \"performance\",\"latency\",\"availability\",\"resilience\",\"cqrs\",\"ddd\",\"clean-architecture\",\n",
        "    \"hexagonal-architecture\",\"pubsub\",\"load-balancing\",\"caching\",\"db-sharding\",\"rate-limiting\"\n",
        "}\n",
        "\n",
        "def has_arch_tag(taglist):\n",
        "    if isinstance(taglist, list):\n",
        "        cleaned = [str(t).lower() for t in taglist if isinstance(t, str)]\n",
        "        return bool(set(cleaned) & ARCH_TAGS)\n",
        "    return False\n",
        "\n",
        "arch_q = questions[questions[\"TagList\"].apply(has_arch_tag)].copy()\n",
        "arch_q[\"Text\"] = (arch_q[\"Title\"].fillna(\"\") + \" \" + arch_q[\"Body\"].fillna(\"\")).str.replace(r\"<[^>]+>\", \" \", regex=True)\n",
        "\n",
        "arch_q = arch_q[[\"QuestionId\",\"Score\",\"Title\",\"Text\",\"TagList\"]].reset_index(drop=True)\n",
        "\n",
        "print(\"Total questions:\", len(questions))\n",
        "print(\"Architecture-like questions:\", len(arch_q))\n",
        "arch_q.head(3)"
      ],
      "metadata": {
        "id": "rWDPzXM-_nuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Build embeddings and FAISS index\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "print(\"Loading embedding model:\", EMB_MODEL_NAME)\n",
        "embed_model = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "def chunk_text(text, max_tokens=250, sep=\"\\n\"):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    parts = text.split(sep)\n",
        "    out, cur, cur_len = [], [], 0\n",
        "    for p in parts:\n",
        "        plen = len(p.split())\n",
        "        if cur_len + plen <= max_tokens:\n",
        "            cur.append(p)\n",
        "            cur_len += plen\n",
        "        else:\n",
        "            if cur:\n",
        "                out.append(sep.join(cur))\n",
        "            cur = [p]\n",
        "            cur_len = plen\n",
        "    if cur:\n",
        "        out.append(sep.join(cur))\n",
        "    final = []\n",
        "    for piece in out:\n",
        "        words = piece.split()\n",
        "        if len(words) <= max_tokens:\n",
        "            final.append(piece)\n",
        "        else:\n",
        "            for i in range(0, len(words), max_tokens):\n",
        "                final.append(\" \".join(words[i:i+max_tokens]))\n",
        "    return final\n",
        "\n",
        "texts, metadatas = [], []\n",
        "for idx, row in tqdm(arch_q.iterrows(), total=len(arch_q), desc=\"Preparing chunks\"):\n",
        "    chunks = chunk_text(row[\"Text\"], max_tokens=200)\n",
        "    if not chunks:\n",
        "        continue\n",
        "    for i, c in enumerate(chunks):\n",
        "        texts.append(c)\n",
        "        metadatas.append({\n",
        "            \"QuestionId\": int(row[\"QuestionId\"]),\n",
        "            \"Title\": row[\"Title\"][:240],\n",
        "            \"TagList\": row[\"TagList\"],\n",
        "            \"Score\": int(row[\"Score\"]) if not pd.isna(row[\"Score\"]) else None,\n",
        "            \"chunk_id\": i\n",
        "        })\n",
        "\n",
        "print(f\"Prepared {len(texts)} text chunks from {len(arch_q)} questions.\")\n",
        "\n",
        "BATCH = 256\n",
        "embeddings = []\n",
        "for i in tqdm(range(0, len(texts), BATCH), desc=\"Embedding batches\"):\n",
        "    batch_texts = texts[i:i+BATCH]\n",
        "    emb = embed_model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True)\n",
        "    embeddings.append(emb)\n",
        "embeddings = np.vstack(embeddings).astype(\"float32\")\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "faiss.normalize_L2(embeddings)\n",
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(embeddings)\n",
        "print(\"FAISS index built. Total vectors:\", index.ntotal)\n",
        "\n",
        "os.makedirs(\"kb\", exist_ok=True)\n",
        "faiss.write_index(index, \"kb/arch_index.faiss\")\n",
        "with open(\"kb/arch_texts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(texts, f)\n",
        "with open(\"kb/arch_metas.pkl\", \"wb\") as f:\n",
        "    pickle.dump(metadatas, f)\n",
        "\n",
        "print(\"Saved index and metadata under ./kb/\")\n"
      ],
      "metadata": {
        "id": "Eu55QOCY_rSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Retrieval helper functions\n",
        "import textwrap, html\n",
        "from IPython.display import display, Markdown\n",
        "from functools import lru_cache\n",
        "\n",
        "if 'index' not in globals():\n",
        "    index = faiss.read_index(\"kb/arch_index.faiss\")\n",
        "if 'texts' not in globals():\n",
        "    with open(\"kb/arch_texts.pkl\",\"rb\") as f:\n",
        "        texts = pickle.load(f)\n",
        "if 'metadatas' not in globals():\n",
        "    with open(\"kb/arch_metas.pkl\",\"rb\") as f:\n",
        "        metadatas = pickle.load(f)\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def retrieve_knn(query, k=5, return_scores=False):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for dist, idx in zip(D[0], I[0]):\n",
        "        if idx < 0 or idx >= len(texts):\n",
        "            continue\n",
        "        meta = metadatas[idx]\n",
        "        snippet = texts[idx]\n",
        "        item = {\n",
        "            \"score\": float(dist),\n",
        "            \"qid\": meta.get(\"QuestionId\"),\n",
        "            \"title\": meta.get(\"Title\"),\n",
        "            \"tags\": meta.get(\"TagList\"),\n",
        "            \"chunk_id\": meta.get(\"chunk_id\"),\n",
        "            \"snippet\": snippet\n",
        "        }\n",
        "        results.append(item if return_scores else {k:v for k,v in item.items() if k!=\"score\"})\n",
        "    return results\n",
        "\n",
        "def show_retrieval(results):\n",
        "    out = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        s = textwrap.shorten(r[\"snippet\"], width=350, placeholder=\"…\")\n",
        "        tags = \", \".join(r[\"tags\"]) if r[\"tags\"] else \"\"\n",
        "        out.append(f\"**[{i}]** — *QID {r['qid']}* — tags: {tags}\\n\\n> {s}\\n\")\n",
        "    display(Markdown(\"\\n\\n\".join(out)))\n",
        "\n",
        "res = retrieve_knn(\"design event-driven microservices with kafka, idempotency, retries\", k=4)\n",
        "show_retrieval(res)"
      ],
      "metadata": {
        "id": "7zNVWemt_t7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Patterns KB (moved to external YAML recommended, but kept inline here)\n",
        "import yaml\n",
        "\n",
        "PATTERNS_YAML = \"\"\"\n",
        "patterns:\n",
        "  - name: Monolith\n",
        "    description: \"Single deployable unit; simple ops; quick to iterate.\"\n",
        "    use_when:\n",
        "      - \"Small team\"\n",
        "      - \"Tight timelines\"\n",
        "    tech_stack:\n",
        "      conservative: [\"Python\", \"FastAPI\", \"PostgreSQL\", \"Redis\"]\n",
        "      balanced: [\"Node.js\", \"Express\", \"Postgres\", \"Redis\"]\n",
        "      innovative: [\"Deno\", \"PlanetScale\", \"Upstash Redis\"]\n",
        "  - name: Microservices\n",
        "    description: \"Multiple services per bounded context; independent deploys.\"\n",
        "    use_when:\n",
        "      - \"Large teams\"\n",
        "      - \"Need independent scaling\"\n",
        "    tech_stack:\n",
        "      conservative: [\"Java\", \"Spring Boot\", \"Postgres\", \"Kafka\", \"Kubernetes\"]\n",
        "      balanced: [\"Go\", \"gRPC\", \"Kafka\", \"MongoDB\", \"Kubernetes\"]\n",
        "      innovative: [\"Rust\", \"NATS\", \"CockroachDB\", \"Service Mesh\"]\n",
        "  - name: Event-Driven\n",
        "    description: \"Asynchronous comms using event bus; good for decoupling.\"\n",
        "    use_when:\n",
        "      - \"High throughput\"\n",
        "      - \"Loose coupling required\"\n",
        "    tech_stack:\n",
        "      conservative: [\"Kafka\", \"Schema Registry\", \"Postgres\"]\n",
        "      balanced: [\"RabbitMQ\", \"Consumers in Python/Go\", \"Elastic sink\"]\n",
        "      innovative: [\"Redpanda\", \"Flink\", \"Delta Lake\"]\n",
        "  - name: Serverless\n",
        "    description: \"Fully managed function-based architecture.\"\n",
        "    use_when:\n",
        "      - \"Spiky workloads\"\n",
        "      - \"Minimal ops\"\n",
        "    tech_stack:\n",
        "      conservative: [\"AWS Lambda\", \"API Gateway\", \"DynamoDB\"]\n",
        "      balanced: [\"GCP Cloud Run + Functions\", \"Firestore\"]\n",
        "      innovative: [\"Dapr\", \"Cloud-native serverless stacks\"]\n",
        "\"\"\"\n",
        "\n",
        "PATTERNS = yaml.safe_load(PATTERNS_YAML)[\"patterns\"]\n",
        "\n",
        "def find_pattern(name):\n",
        "    name = name.lower()\n",
        "    for p in PATTERNS:\n",
        "        if name in p[\"name\"].lower():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def list_patterns():\n",
        "    return [p[\"name\"] for p in PATTERNS]\n",
        "\n",
        "print(\"Available patterns:\", list_patterns())"
      ],
      "metadata": {
        "id": "fmqwOeob6CmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Initialize InferenceClient (reuse token)\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "hf_client = InferenceClient(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
        ")"
      ],
      "metadata": {
        "id": "HOOVcrEh_2V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Setup Hugging Face Client and Define Functions for Cleaning & Generating Proposals\n",
        "\n",
        "import re\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "hf_client = InferenceClient(model=MODEL_ID, token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
        "\n",
        "def _clean_llm_text(text: str) -> str:\n",
        "    \"\"\"Clean and de-duplicate LLM output, collapse repeated adjacent phrases,\n",
        "    limit bullets per section, and produce tidy Markdown.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # 1) Remove common unwanted markers and control tokens\n",
        "    text = re.sub(r\"\\[/?ASSIST(?:ANT)?\\]\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"<\\/?s>\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\[/?USER\\]\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\[/?SYSTEM\\]\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2) Normalize whitespace and line endings\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)  # no huge gaps\n",
        "\n",
        "    # 3) Break into lines, strip, remove empty lines\n",
        "    lines = [ln.strip() for ln in text.split(\"\\n\")]\n",
        "    lines = [ln for ln in lines if ln != \"\"]\n",
        "\n",
        "    # 4) Collapse long repeated adjacent lines (e.g., the same operator repeated)\n",
        "    collapsed = []\n",
        "    prev = None\n",
        "    for ln in lines:\n",
        "        if ln == prev:\n",
        "            # skip exact adjacent duplicate\n",
        "            continue\n",
        "        collapsed.append(ln)\n",
        "        prev = ln\n",
        "    lines = collapsed\n",
        "\n",
        "    # 5) Deduplicate while preserving order (but keep short duplicates within same section)\n",
        "    seen = set()\n",
        "    deduped = []\n",
        "    for ln in lines:\n",
        "        key = ln.lower()\n",
        "        # For long lines we keep if new; for tiny repeated tokens also dedupe\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        deduped.append(ln)\n",
        "    lines = deduped\n",
        "\n",
        "    # 6) Re-scan for headings and group sections, truncating bullet lists per section\n",
        "    # Recognize headings like \"1.\", \"1)\", \"1 -\", or Markdown bold headings (**Recommended Pattern**)\n",
        "    sections = []\n",
        "    cur_title = \"Intro\"\n",
        "    cur_lines = []\n",
        "    heading_re = re.compile(r\"^\\s*(\\d+[\\.\\)\\-]|#+\\s+|\\*\\*.+\\*\\*|[A-Za-z ]+:)\\s*\")\n",
        "    for ln in lines:\n",
        "        if heading_re.match(ln):\n",
        "            # flush previous\n",
        "            if cur_lines:\n",
        "                sections.append((cur_title, cur_lines))\n",
        "            # treat the heading as the new title (clean it)\n",
        "            title = ln\n",
        "            title = re.sub(r\"^\\d+[\\.\\)\\-]\\s*\", \"\", title)  # remove leading number\n",
        "            title = title.strip()\n",
        "            cur_title = title if title else \"Section\"\n",
        "            cur_lines = []\n",
        "        else:\n",
        "            cur_lines.append(ln)\n",
        "    if cur_lines:\n",
        "        sections.append((cur_title, cur_lines))\n",
        "\n",
        "    # 7) For each section, keep at most N bullets and collapse long repetitive sentences into bullets\n",
        "    MAX_BULLETS = 6\n",
        "    out_sections = []\n",
        "    for title, s_lines in sections:\n",
        "        # Convert lines into bullets: split by common bullet markers if present\n",
        "        bullets = []\n",
        "        for ln in s_lines:\n",
        "            # if line starts with a dash / bullet, strip it\n",
        "            if re.match(r\"^[-•\\*\\u2022]\\s+\", ln):\n",
        "                ln = re.sub(r\"^[-•\\*\\u2022]\\s+\", \"\", ln)\n",
        "            # Split comma-heavy sentences into separate bullets only if clearly enumerative and short\n",
        "            if len(ln) > 200:\n",
        "                # long paragraph — keep as single bullet but shorten\n",
        "                ln = re.sub(r\"\\s+\", \" \", ln)  # collapse internal whitespace\n",
        "            bullets.append(ln)\n",
        "\n",
        "        # dedupe bullets while preserving order\n",
        "        seen_b = set()\n",
        "        uniq_bullets = []\n",
        "        for b in bullets:\n",
        "            key = b.lower()\n",
        "            if key in seen_b:\n",
        "                continue\n",
        "            seen_b.add(key)\n",
        "            uniq_bullets.append(b)\n",
        "\n",
        "        # truncate\n",
        "        uniq_bullets = uniq_bullets[:MAX_BULLETS]\n",
        "\n",
        "        # If no explicit bullets, try splitting long lines into sensible bullets using sentences.\n",
        "        if len(uniq_bullets) == 1 and len(uniq_bullets[0]) > 300:\n",
        "            spl = re.split(r\"(?<=\\.)\\s+\", uniq_bullets[0])\n",
        "            uniq_bullets = [s.strip() for s in spl if s.strip()][:MAX_BULLETS]\n",
        "\n",
        "        out_sections.append((title, uniq_bullets))\n",
        "\n",
        "    # 8) Reconstruct Markdown output with titles and bullets\n",
        "    md_parts = []\n",
        "    for title, bullets in out_sections:\n",
        "        if title.lower() != \"intro\":\n",
        "            md_parts.append(f\"### {title.strip()}\")\n",
        "        for b in bullets:\n",
        "            # ensure bullet starts with \"- \"\n",
        "            md_parts.append(f\"- {b}\")\n",
        "        md_parts.append(\"\")  # blank line between sections\n",
        "\n",
        "    out_md = \"\\n\".join(md_parts).strip()\n",
        "    # Final cleanup: avoid duplicated blank lines\n",
        "    out_md = re.sub(r\"\\n{3,}\", \"\\n\\n\", out_md)\n",
        "    return out_md\n",
        "\n",
        "def generate_proposals(requirement: str, k: int = 3) -> str:\n",
        "    # retrieve and assemble context\n",
        "    retrieved = retrieve_knn(requirement, k=k, return_scores=True)\n",
        "    context = \"\\n\\n\".join([f\"- {r['snippet']}\" for r in retrieved])\n",
        "\n",
        "    # stricter system prompt that tells the model to be concise and limited\n",
        "    system_prompt = \"\"\"\n",
        "You are an expert cloud architect.\n",
        "Always respond in a concise Markdown format with headings and bullets.\n",
        "Keep each section short (max ~6 bullets). Never repeat the same advice.\n",
        "If the user asks for optimizations, provide at most 5 practical, non-overlapping tips.\n",
        "Do not enumerate low-level operators or repeat identical commands.\n",
        "\"\"\".strip()\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"Requirement: {requirement}\\n\\n\"\n",
        "        f\"Context snippets:\\n{context}\\n\\n\"\n",
        "        \"Provide a structured recommendation using headings and bullets.\"\n",
        "    )\n",
        "\n",
        "    response = hf_client.chat_completion(\n",
        "        model=MODEL_ID,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=1024,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    raw = response.choices[0].message.get(\"content\", \"\")\n",
        "    cleaned = _clean_llm_text(raw)\n",
        "    # If cleaning produced nothing, fall back to raw (cleaned minimally)\n",
        "    return cleaned if cleaned else raw\n"
      ],
      "metadata": {
        "id": "7hSy0vGg_Rhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step12: Interactive Chat Assistant Loop (Markdown display)\n",
        "from IPython.display import Markdown, display\n",
        "import textwrap\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def chat_with_architect():\n",
        "    global chat_history\n",
        "    print(\"🤖 Conversational Code Architecture Assistant (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"👋 Ending session.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            rec_md = generate_proposals(user_input, k=3)\n",
        "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": rec_md})\n",
        "\n",
        "            # Render as Markdown for clean headings & bullets\n",
        "            display(Markdown(rec_md))\n",
        "            print(\"\\n\" + \"-\"*100 + \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error generating response: {e}\")\n",
        "            break\n",
        "\n",
        "# Start the chat\n",
        "chat_with_architect()\n"
      ],
      "metadata": {
        "id": "_p_fBqyK6cen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}